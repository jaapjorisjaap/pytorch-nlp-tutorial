{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set default to run on the GPU if available (for the speed up)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imdb sentiment analysis.\n",
    "\n",
    "This time we are going to look at imdb sentiment analysis. The imdb dataset is a dataset containing movie reviews and a label for each review wheter it is positive or negative. We are going to create a model that will predict if a review is positive or negative. \n",
    "\n",
    "We will make use of torchtext for loading and proprocessing the dataset.\n",
    "\n",
    "Read and run the following cell (this can take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\imdb\\aclImdb_v1.tar.gz: 100%|████████████████████████████████████████████████| 84.1M/84.1M [03:06<00:00, 452kB/s]\n"
     ]
    }
   ],
   "source": [
    "# First we setup the imdb dataset\n",
    "from torchtext import data, datasets\n",
    "# set up fields, one for the text in the review and one for the label. \n",
    "# We will make each review of length 100 (Set to a smaller number for faster training)\n",
    "TEXT = data.Field(lower=True, batch_first=True, fix_length=100)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "# load the dataset. This prompts a download, which will take a minute or 2\n",
    "train, test = datasets.IMDB.splits(TEXT, LABEL) \n",
    "\n",
    "# build the vocabulary. We will only use the 1000 most common words. All the other words will be mapped to the <unk> token\n",
    "TEXT.build_vocab(train, max_size=1000)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "# make iterator for splits\n",
    "train_iter, test_iter = data.Iterator.splits((train, test), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's eplore the things that we have created.\n",
    "\n",
    "First the vocabulairy. This can be seen as a dictonairy that maps each possible word to a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "97\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# A word that it does know\n",
    "print(TEXT.vocab['good'])\n",
    "print(TEXT.vocab['bad'])\n",
    "# An unknown word\n",
    "print(TEXT.vocab[\"notaword\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we have the iterator. We will print one batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 64 from IMDB]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 64x100 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.LongTensor of size 64 (GPU 0)]\n",
      "tensor([   9,  226,    0,    5,   48,    0,    0,   14,  695,    8,    2,  155,\n",
      "           4,   81,    2,    0,    5,    2,    0,   11,   27,   14,    0,    6,\n",
      "          28,    0,   46,   10,    7,    0,  427,   19,    0,    0,    0,    9,\n",
      "         226,    0,   11,   12,    7,    3,    0,    0,    9,  226,   81,    0,\n",
      "           5,    2,    0,    0,   27,   81,   95,    3, 1000,    8,    2,  435,\n",
      "           0,   20,   43,    2,    0,   11,    2,    0,  160,  347,  101,  316,\n",
      "          11,    0,    6,   21,    0,    0,    0,   11,   27,  199,   12,   14,\n",
      "           2,  116,    0,    0,  125,    0,    9,   54,   37,    6,  118,   63,\n",
      "           0,   11,    7,    0])\n",
      "tensor([2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1,\n",
      "        1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2,\n",
      "        1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "#### explore the output of the iterator\n",
    "\n",
    "for i, batch in enumerate(train_iter):\n",
    "    print(batch)\n",
    "    print(batch.text[0])\n",
    "    print(batch.label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the text is now a tensor containing a number for each word.\n",
    "The label are 1 -> negative and 2 -> positive. We will substract 1 during training to make sure it becomes 0 and 1. \n",
    "\n",
    "Now that we have the data al set up we can build a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "Build a simple lstm model, using an embedding layer as the first layer, then a recurrent layer (lstm or gru) and to finish a fully connected layer that maps to 1 value, and finally use a sigmoid to make sure that this value is between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    ### your code here ###\n",
    "   \n",
    "    ### your code here end ###\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gerso\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:57: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embedding): Embedding(1004, 128)\n",
       "  (LSTM): GRU(128, 128, batch_first=True, dropout=0.2)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "\n",
    "### your code here ###\n",
    "\n",
    "### your code here end ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A train and test function made for you. \n",
    "def train(train_iterator, model, criterion, optimizer, n_epochs, test_callback=None):\n",
    "    loss_history = []\n",
    "    test_history = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_total_loss = 0\n",
    "        \n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "\n",
    "            output = model(batch.text)\n",
    "\n",
    "            loss = criterion(output, (batch.label).float().view(-1, 1) - 1) ### Make sure the labels are either 0 or 1. \n",
    "\n",
    "            loss.backward() # Does backpropagation and calculates gradients\n",
    "            optimizer.step() # Updates the weights accordingly\n",
    "\n",
    "            epoch_total_loss += loss.item() # Keep track of the total loss\n",
    "            \n",
    "            #Caclucate the accuracy\n",
    "            total += len(batch)\n",
    "            predicted = torch.round(output.data)\n",
    "            correct += (predicted == (batch.label -1).view(-1, 1)).sum().item()\n",
    "            \n",
    "        loss_history.append(epoch_total_loss/len(train_iter))\n",
    "        \n",
    "        if test_callback != None:\n",
    "            test_history.append(test_callback(model, criterion))\n",
    "\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(epoch_total_loss/ len(train_iter)))\n",
    "        print(\"accuracy: {:.4f}\".format(correct/total))\n",
    "    return loss_history, test_history\n",
    "\n",
    "\n",
    "def test(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            output = model(batch.text)\n",
    "\n",
    "            loss = criterion(output, (batch.label - 1).float().view(-1, 1))\n",
    "            total_loss += loss.item()\n",
    "            predicted = torch.round(output.data)\n",
    "    \n",
    "            total += len(batch)\n",
    "            \n",
    "            correct += (predicted == (batch.label -1).view(-1, 1)).sum().item()\n",
    "        print(\"Test loss: {:.4f}\".format(total_loss/len(dataloader)))\n",
    "        print(\"test accuracy: {:.4f}\".format(correct/total))\n",
    "    model.train()\n",
    "    return total_loss/len(dataloader)\n",
    "            \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6915\n",
      "test accuracy: 0.5267\n",
      "Epoch: 1/20............. Loss: 0.6925\n",
      "accuracy: 0.5207\n",
      "Test loss: 0.6872\n",
      "test accuracy: 0.5448\n",
      "Epoch: 2/20............. Loss: 0.6873\n",
      "accuracy: 0.5472\n",
      "Test loss: 0.6656\n",
      "test accuracy: 0.6028\n",
      "Epoch: 3/20............. Loss: 0.6745\n",
      "accuracy: 0.5822\n",
      "Test loss: 0.6170\n",
      "test accuracy: 0.6700\n",
      "Epoch: 4/20............. Loss: 0.6309\n",
      "accuracy: 0.6516\n",
      "Test loss: 0.5669\n",
      "test accuracy: 0.7129\n",
      "Epoch: 5/20............. Loss: 0.5782\n",
      "accuracy: 0.7017\n",
      "Test loss: 0.5389\n",
      "test accuracy: 0.7304\n",
      "Epoch: 6/20............. Loss: 0.5347\n",
      "accuracy: 0.7342\n",
      "Test loss: 0.5250\n",
      "test accuracy: 0.7404\n",
      "Epoch: 7/20............. Loss: 0.5075\n",
      "accuracy: 0.7525\n",
      "Test loss: 0.5108\n",
      "test accuracy: 0.7496\n",
      "Epoch: 8/20............. Loss: 0.4854\n",
      "accuracy: 0.7672\n",
      "Test loss: 0.4969\n",
      "test accuracy: 0.7560\n",
      "Epoch: 9/20............. Loss: 0.4688\n",
      "accuracy: 0.7758\n",
      "Test loss: 0.4995\n",
      "test accuracy: 0.7580\n",
      "Epoch: 10/20............. Loss: 0.4562\n",
      "accuracy: 0.7838\n",
      "Test loss: 0.4831\n",
      "test accuracy: 0.7652\n",
      "Epoch: 11/20............. Loss: 0.4465\n",
      "accuracy: 0.7911\n",
      "Test loss: 0.4883\n",
      "test accuracy: 0.7638\n",
      "Epoch: 12/20............. Loss: 0.4327\n",
      "accuracy: 0.7981\n",
      "Test loss: 0.4786\n",
      "test accuracy: 0.7707\n",
      "Epoch: 13/20............. Loss: 0.4279\n",
      "accuracy: 0.8018\n",
      "Test loss: 0.4743\n",
      "test accuracy: 0.7724\n",
      "Epoch: 14/20............. Loss: 0.4162\n",
      "accuracy: 0.8074\n",
      "Test loss: 0.4944\n",
      "test accuracy: 0.7674\n",
      "Epoch: 15/20............. Loss: 0.4087\n",
      "accuracy: 0.8133\n",
      "Test loss: 0.4909\n",
      "test accuracy: 0.7624\n",
      "Epoch: 16/20............. Loss: 0.4034\n",
      "accuracy: 0.8156\n",
      "Test loss: 0.4747\n",
      "test accuracy: 0.7749\n",
      "Epoch: 17/20............. Loss: 0.3958\n",
      "accuracy: 0.8171\n",
      "Test loss: 0.4759\n",
      "test accuracy: 0.7707\n",
      "Epoch: 18/20............. Loss: 0.3903\n",
      "accuracy: 0.8209\n",
      "Test loss: 0.4790\n",
      "test accuracy: 0.7765\n",
      "Epoch: 19/20............. Loss: 0.3819\n",
      "accuracy: 0.8248\n",
      "Test loss: 0.4849\n",
      "test accuracy: 0.7714\n",
      "Epoch: 20/20............. Loss: 0.3728\n",
      "accuracy: 0.8315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.692466125006566,\n",
       "  0.6872996508008073,\n",
       "  0.6745399485158798,\n",
       "  0.630903163529418,\n",
       "  0.5781750868805839,\n",
       "  0.5347197629759074,\n",
       "  0.5075281070321417,\n",
       "  0.4854333762012784,\n",
       "  0.46878297554562465,\n",
       "  0.4562239401480731,\n",
       "  0.44645765355176026,\n",
       "  0.43265173349843916,\n",
       "  0.42788938907406215,\n",
       "  0.4162208127319965,\n",
       "  0.40872080006715283,\n",
       "  0.4033765041690958,\n",
       "  0.395789208924374,\n",
       "  0.39031493568511877,\n",
       "  0.38187781795668785,\n",
       "  0.3728023030416435],\n",
       " [0.6914828742861443,\n",
       "  0.6872391164150384,\n",
       "  0.6656061054190712,\n",
       "  0.6169612239236417,\n",
       "  0.566850199571351,\n",
       "  0.5389149401651319,\n",
       "  0.5249890650782134,\n",
       "  0.5108112409672774,\n",
       "  0.49685966328281883,\n",
       "  0.4994553518493462,\n",
       "  0.4830516484539832,\n",
       "  0.48829825919912295,\n",
       "  0.47860247617030083,\n",
       "  0.47432800590077323,\n",
       "  0.49444912225389115,\n",
       "  0.4909029400257198,\n",
       "  0.4746713745395851,\n",
       "  0.4758910871756351,\n",
       "  0.47896208985687216,\n",
       "  0.4849302517559827])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the train and test functions with the appropiate inputs.\n",
    "\n",
    "### your code here ###\n",
    "\n",
    "### your code here end ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we look at the result of the model. \n",
    "\n",
    "\n",
    "Make a function that given a review and your trained model, gives back a score on how positive it was. 0 being negative and 1 being positive.\n",
    "\n",
    "Make sure to call model.eval() and model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def predict(review, model):\n",
    "    ### your code here ###\n",
    "\n",
    "    ### your code here end ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3583]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"bad\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"excellent movie\",\n",
    "    \"i really liked this movie \",\n",
    "    \"this movie really sucked !\",\n",
    "    \"bad movie!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excellent movie tensor([[0.7028]], grad_fn=<SigmoidBackward>) \n",
      "\n",
      "i really liked this movie  tensor([[0.6523]], grad_fn=<SigmoidBackward>) \n",
      "\n",
      "this movie really sucked ! tensor([[0.4710]], grad_fn=<SigmoidBackward>) \n",
      "\n",
      "bad movie! tensor([[0.3076]], grad_fn=<SigmoidBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for review in reviews:\n",
    "    print(review, predict(review, model) , '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the model\n",
    "\n",
    "Next up you will be improving this model\n",
    "\n",
    "For the embedding layer we will use the a pretrained word embedding called glove. See: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "We will first load glove into word vectors and then preprocess the dataset with these vectors. This is needed because each word must map to the same index that is used in the glove word embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████▉| 399999/400000 [00:40<00:00, 9998.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "glove = Vectors(name=\"../glove/glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "\n",
    "# set up fields again.\n",
    "TEXT = data.Field(lower=True, batch_first=True, fix_length=100)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train_set, test_set = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# build the vocabulary, this time with help of the glove vectors. \n",
    "#We provide the vectors, such that we only add words that we have a vector for in our vocab\n",
    "TEXT.build_vocab(train_set, vectors=glove, max_size=1000)\n",
    "LABEL.build_vocab(train_set)\n",
    "\n",
    "# make iterator for splits\n",
    "train_iter_glove, test_iter_glove = data.Iterator.splits(\n",
    "    (train_set, test_set), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the output of the vocab. Notice that the indexes are still the same, that is because they are sorted on the frequency of occurences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "97\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# A word that it does know\n",
    "print(TEXT.vocab['good'])\n",
    "print(TEXT.vocab['bad'])\n",
    "# An unknown word\n",
    "print(TEXT.vocab[\"notaword\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the output of the iterator. This has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 64 from IMDB]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 64x100 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.LongTensor of size 64 (GPU 0)]\n",
      "tensor([[ 10,   7, 382,  ...,   4,  41,  45],\n",
      "        [ 10,   0,   5,  ...,   0,   0,   5],\n",
      "        [  9,  37, 123,  ...,   7,   0,   0],\n",
      "        ...,\n",
      "        [  9, 178, 250,  ..., 614,   8,   0],\n",
      "        [ 50,   9,  62,  ...,   0,   0,  46],\n",
      "        [  9,  98,  11,  ...,   1,   1,   1]])\n",
      "torch.Size([64, 100])\n",
      "tensor([2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2,\n",
      "        1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2,\n",
      "        1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iter):\n",
    "    print(batch)\n",
    "    print(batch.text)\n",
    "    print(batch.text.shape)\n",
    "    print(batch.label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the improved model\n",
    "\n",
    "Build the improved model. Use the same architecture as before, but instead of a standard word embedding vector use the glove word embedding vector.\n",
    "\n",
    "See https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding for a hint on how to use the pretrained glove. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class GloveModel(nn.Module):\n",
    "    ### your code here ###\n",
    "\n",
    "    ### your code here end ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GloveModel(\n",
       "  (emb): Embedding(1002, 100)\n",
       "  (gru): GRU(100, 128, batch_first=True)\n",
       "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create your model\n",
    "\n",
    "### your code here ###\n",
    "\n",
    "### your code here end ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4788\n",
      "test accuracy: 0.7664\n",
      "Epoch: 1/20............. Loss: 0.5547\n",
      "accuracy: 0.6992\n",
      "Test loss: 0.4784\n",
      "test accuracy: 0.7718\n",
      "Epoch: 2/20............. Loss: 0.4679\n",
      "accuracy: 0.7747\n",
      "Test loss: 0.5008\n",
      "test accuracy: 0.7505\n",
      "Epoch: 3/20............. Loss: 0.4635\n",
      "accuracy: 0.7797\n",
      "Test loss: 0.5022\n",
      "test accuracy: 0.7533\n",
      "Epoch: 4/20............. Loss: 0.4749\n",
      "accuracy: 0.7715\n",
      "Test loss: 0.4897\n",
      "test accuracy: 0.7624\n",
      "Epoch: 5/20............. Loss: 0.4655\n",
      "accuracy: 0.7751\n",
      "Test loss: 0.5074\n",
      "test accuracy: 0.7470\n",
      "Epoch: 6/20............. Loss: 0.4726\n",
      "accuracy: 0.7715\n",
      "Test loss: 0.5083\n",
      "test accuracy: 0.7490\n",
      "Epoch: 7/20............. Loss: 0.4814\n",
      "accuracy: 0.7626\n",
      "Test loss: 0.5055\n",
      "test accuracy: 0.7501\n",
      "Epoch: 8/20............. Loss: 0.4796\n",
      "accuracy: 0.7661\n",
      "Test loss: 0.4985\n",
      "test accuracy: 0.7548\n",
      "Epoch: 9/20............. Loss: 0.4696\n",
      "accuracy: 0.7752\n",
      "Test loss: 0.4921\n",
      "test accuracy: 0.7585\n",
      "Epoch: 10/20............. Loss: 0.4642\n",
      "accuracy: 0.7792\n",
      "Test loss: 0.5081\n",
      "test accuracy: 0.7490\n",
      "Epoch: 11/20............. Loss: 0.4636\n",
      "accuracy: 0.7768\n",
      "Test loss: 0.5071\n",
      "test accuracy: 0.7494\n",
      "Epoch: 12/20............. Loss: 0.4720\n",
      "accuracy: 0.7729\n",
      "Test loss: 0.5210\n",
      "test accuracy: 0.7419\n",
      "Epoch: 13/20............. Loss: 0.4766\n",
      "accuracy: 0.7692\n",
      "Test loss: 0.5187\n",
      "test accuracy: 0.7471\n",
      "Epoch: 14/20............. Loss: 0.4840\n",
      "accuracy: 0.7654\n",
      "Test loss: 0.5377\n",
      "test accuracy: 0.7298\n",
      "Epoch: 15/20............. Loss: 0.5036\n",
      "accuracy: 0.7533\n",
      "Test loss: 0.5385\n",
      "test accuracy: 0.7360\n",
      "Epoch: 16/20............. Loss: 0.4971\n",
      "accuracy: 0.7576\n",
      "Test loss: 0.5373\n",
      "test accuracy: 0.7374\n",
      "Epoch: 17/20............. Loss: 0.4887\n",
      "accuracy: 0.7635\n",
      "Test loss: 0.5158\n",
      "test accuracy: 0.7445\n",
      "Epoch: 18/20............. Loss: 0.4784\n",
      "accuracy: 0.7671\n",
      "Test loss: 0.5236\n",
      "test accuracy: 0.7445\n",
      "Epoch: 19/20............. Loss: 0.4735\n",
      "accuracy: 0.7702\n",
      "Test loss: 0.5171\n",
      "test accuracy: 0.7460\n",
      "Epoch: 20/20............. Loss: 0.4783\n",
      "accuracy: 0.7698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5546873286557015,\n",
       "  0.4679495672435712,\n",
       "  0.4634969122422016,\n",
       "  0.47491328437310043,\n",
       "  0.46554286515011506,\n",
       "  0.4725953018878732,\n",
       "  0.4814195406558873,\n",
       "  0.4795529649538152,\n",
       "  0.4695868889998902,\n",
       "  0.4642304591358165,\n",
       "  0.46360078926586434,\n",
       "  0.47200196303065173,\n",
       "  0.47655868568383825,\n",
       "  0.4839686896185131,\n",
       "  0.5036174357699617,\n",
       "  0.4970815979763675,\n",
       "  0.48870639628766444,\n",
       "  0.4784226937367178,\n",
       "  0.4735247378459062,\n",
       "  0.4783393055429239],\n",
       " [0.47880952913895286,\n",
       "  0.4784197878197331,\n",
       "  0.5007757043747036,\n",
       "  0.5022006344307414,\n",
       "  0.4896800354161226,\n",
       "  0.5073573013095904,\n",
       "  0.5082996977717066,\n",
       "  0.5055322560203045,\n",
       "  0.4984945202117686,\n",
       "  0.49206945902246346,\n",
       "  0.5080820442465566,\n",
       "  0.5070649440712331,\n",
       "  0.5209832135063913,\n",
       "  0.5187466839314117,\n",
       "  0.5376601321313083,\n",
       "  0.5384633570071071,\n",
       "  0.5372882766644363,\n",
       "  0.5157905725566932,\n",
       "  0.5235775001061237,\n",
       "  0.5171376735048221])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finaly train the model\n",
    "\n",
    "### your code here ###\n",
    "\n",
    "### your code here end ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we will evualate this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes in a review and your glove model and gives back the sentiment score. \n",
    "def predict_glove(review, model):\n",
    "    ### Your code here ###\n",
    "\n",
    "    ### Your code here end ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"excellent movie\",\n",
    "    \"I really liked this movie !\",\n",
    "    \"This movie is really bad !\",\n",
    "    \"best movie ever!\",\n",
    "    \"A Eggsellent movie\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excellent movie tensor([[0.9883]], grad_fn=<SigmoidBackward>) \n",
      "\n",
      "I really liked this movie ! tensor([[0.8884]], grad_fn=<SigmoidBackward>) \n",
      "\n",
      "This movie is really bad ! tensor([[0.0405]], grad_fn=<SigmoidBackward>) \n",
      "\n",
      "best movie ever! tensor([[0.9424]], grad_fn=<SigmoidBackward>) \n",
      "\n",
      "A Eggsellent movie tensor([[0.5038]], grad_fn=<SigmoidBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for review in reviews:\n",
    "    print(review, predict_glove(review, m) , '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the output? Compare it to the previous model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
